{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nWhat we did right here is important to conceptually understand before moving on. We are importing both training data and test data.\\nWe defined \"Transform\" earlier which is how we are going to take these 28x28 images and convert them into tensors. We then normalize\\nthe data so that the values are between -1 and 1. We then check if we have a GPU available and if we do, we use it. We then load the\\ntraining and test datasets. We then create a DataLoader for both the training and test datasets. The DataLoader is what we use to\\nactually load the data in batches. We can specify the batch size and whether or not we want to shuffle the data. We shuffle the data\\nfor the training set but not for the test set.\\n\\nThe reason we shuffle the training data is because we want to make sure that the model doesn\\'t learn the order of the data. If the\\nmodel learns the order of the data, it will not generalize well to new data. We don\\'t shuffle the test data because we want to make\\nsure that the model is able to generalize to new data. If we shuffle the test data, we won\\'t be able to evaluate the model\\'s performance\\n\\nTraining data is used to \"train\" the model. This means it is used by the learning algo to learn the parameters or weights \\nthat will define the model. This training data typically consits of input and output pairs. This means it would have the input\\ndata and the corresponding output data. The model is trained to learn the relationship between the input and output data.\\n\\nIt is important that the test data is not used during the training process. The test data is used to evaluate the model\\'s performance\\n'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch # This is working with pytorch which is a deep learning library\n",
    "import torchvision # This is mainly for working with image based stuff\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "# Define a transform to normalize the data\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5,), (0.5,))\n",
    "])\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu') # Deciding whether or not we're going to be using the CPU or the GPU\n",
    "\n",
    "# Load the training and test datasets\n",
    "trainset = torchvision.datasets.MNIST(root='./data', train=True, download=True, transform=transform)\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=64, shuffle=True)\n",
    "\n",
    "testset = torchvision.datasets.MNIST(root='./data', train=False, download=True, transform=transform)\n",
    "testloader = torch.utils.data.DataLoader(testset, batch_size=64, shuffle=False)\n",
    "\n",
    "'''\n",
    "What we did right here is important to conceptually understand before moving on. We are importing both training data and test data.\n",
    "We defined \"Transform\" earlier which is how we are going to take these 28x28 images and convert them into tensors. We then normalize\n",
    "the data so that the values are between -1 and 1. We then check if we have a GPU available and if we do, we use it. We then load the\n",
    "training and test datasets. We then create a DataLoader for both the training and test datasets. The DataLoader is what we use to\n",
    "actually load the data in batches. We can specify the batch size and whether or not we want to shuffle the data. We shuffle the data\n",
    "for the training set but not for the test set.\n",
    "\n",
    "The reason we shuffle the training data is because we want to make sure that the model doesn't learn the order of the data. If the\n",
    "model learns the order of the data, it will not generalize well to new data. We don't shuffle the test data because we want to make\n",
    "sure that the model is able to generalize to new data. If we shuffle the test data, we won't be able to evaluate the model's performance\n",
    "\n",
    "Training data is used to \"train\" the model. This means it is used by the learning algo to learn the parameters or weights \n",
    "that will define the model. This training data typically consits of input and output pairs. This means it would have the input\n",
    "data and the corresponding output data. The model is trained to learn the relationship between the input and output data.\n",
    "\n",
    "It is important that the test data is not used during the training process. The test data is used to evaluate the model's performance\n",
    "'''\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Net(\n",
       "  (fc1): Linear(in_features=784, out_features=512, bias=True)\n",
       "  (fc2): Linear(in_features=512, out_features=256, bias=True)\n",
       "  (fc3): Linear(in_features=256, out_features=10, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        # FC = Fully connected layer\n",
    "        self.fc1 = nn.Linear(28 * 28, 512)  # The images are 28 x 28\n",
    "        self.fc2 = nn.Linear(512, 256) # 512 input features from previous layer and maps to 256 neurons\n",
    "        self.fc3 = nn.Linear(256, 10) # This maps the 256 inputs it gets to the 10 outputs\n",
    "        # It seems like the final layer is the output layer which goes to the total types of outputs\n",
    "\n",
    "        # For something like true false, would I only have 2 outputs?\n",
    "    def forward(self, x):\n",
    "        # This is how the data flows through the layers. We take the 2D image which is 28 x 28 and flatten it to 1D\n",
    "        x = x.view(-1, 28 * 28)  # Flatten the input\n",
    "\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "net = Net() # Making a new object of the \"net\" type with is a subset of the Net Super class\n",
    "net.to(device) # Sending this to the right device ( adding it to my GPU if possible )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nlr= learning rate. In this case, when we set the learning rate to 0.01, it controls the \"step size\" at each iteration while moving towards a minimum of he loss function\\nThe step size is based on how fast or slow the model learns. A larger learning rate means larger updates to the weights while a smaller learning rate\\nmeans smaller updates. From ChatGPT, here are the pros and cons to both:\\nA large learning rate can make the training process faster because the model makes significant changes to the parameters in each step. However, if it\\'s too large, it can cause the model to overshoot the optimal solution, potentially causing the loss to diverge.\\nA small learning rate ensures that the model makes incremental updates to the parameters, which can lead to more precise convergence. However, it can make the training process slow and might get stuck in local minima.\\n'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch.optim as optim # We are using this to try and optimize the model\n",
    "\n",
    "criterion = nn.CrossEntropyLoss() \n",
    "# This is the loss function. This is used to calculate the error of the model. The CrossEntropyLoss is used for classification problems\n",
    "# basically, cross entropy loss is used for putting stuff into multiple different groups, which is something that we want, because\n",
    "# in our case, we're trying to split images of handwritten digits into 10 different groups (0-9)\n",
    "optimizer = optim.SGD(net.parameters(), lr=0.01, momentum=0.9)\n",
    "\n",
    "'''\n",
    "lr= learning rate. In this case, when we set the learning rate to 0.01, it controls the \"step size\" at each iteration while moving towards a minimum of he loss function\n",
    "The step size is based on how fast or slow the model learns. A larger learning rate means larger updates to the weights while a smaller learning rate\n",
    "means smaller updates. From ChatGPT, here are the pros and cons to both:\n",
    "A large learning rate can make the training process faster because the model makes significant changes to the parameters in each step. However, if it's too large, it can cause the model to overshoot the optimal solution, potentially causing the loss to diverge.\n",
    "A small learning rate ensures that the model makes incremental updates to the parameters, which can lead to more precise convergence. However, it can make the training process slow and might get stuck in local minima.\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1,   100] loss: 1.211\n",
      "Epoch [1,   200] loss: 0.424\n",
      "Epoch [1,   300] loss: 0.353\n",
      "Epoch [1,   400] loss: 0.334\n",
      "Epoch [1,   500] loss: 0.295\n",
      "Epoch [1,   600] loss: 0.271\n",
      "Epoch [1,   700] loss: 0.248\n",
      "Epoch [1,   800] loss: 0.214\n",
      "Epoch [1,   900] loss: 0.217\n",
      "Epoch [2,   100] loss: 0.186\n",
      "Epoch [2,   200] loss: 0.175\n",
      "Epoch [2,   300] loss: 0.184\n",
      "Epoch [2,   400] loss: 0.153\n",
      "Epoch [2,   500] loss: 0.128\n",
      "Epoch [2,   600] loss: 0.158\n",
      "Epoch [2,   700] loss: 0.142\n",
      "Epoch [2,   800] loss: 0.138\n",
      "Epoch [2,   900] loss: 0.135\n",
      "Epoch [3,   100] loss: 0.117\n",
      "Epoch [3,   200] loss: 0.116\n",
      "Epoch [3,   300] loss: 0.104\n",
      "Epoch [3,   400] loss: 0.115\n",
      "Epoch [3,   500] loss: 0.098\n",
      "Epoch [3,   600] loss: 0.103\n",
      "Epoch [3,   700] loss: 0.106\n",
      "Epoch [3,   800] loss: 0.103\n",
      "Epoch [3,   900] loss: 0.108\n",
      "Epoch [4,   100] loss: 0.096\n",
      "Epoch [4,   200] loss: 0.091\n",
      "Epoch [4,   300] loss: 0.086\n",
      "Epoch [4,   400] loss: 0.079\n",
      "Epoch [4,   500] loss: 0.078\n",
      "Epoch [4,   600] loss: 0.077\n",
      "Epoch [4,   700] loss: 0.081\n",
      "Epoch [4,   800] loss: 0.076\n",
      "Epoch [4,   900] loss: 0.097\n",
      "Epoch [5,   100] loss: 0.064\n",
      "Epoch [5,   200] loss: 0.067\n",
      "Epoch [5,   300] loss: 0.064\n",
      "Epoch [5,   400] loss: 0.071\n",
      "Epoch [5,   500] loss: 0.069\n",
      "Epoch [5,   600] loss: 0.065\n",
      "Epoch [5,   700] loss: 0.065\n",
      "Epoch [5,   800] loss: 0.063\n",
      "Epoch [5,   900] loss: 0.071\n",
      "Finished Training\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 5\n",
    "\n",
    "'''\n",
    "\n",
    "\n",
    "\n",
    "'''\n",
    "for epoch in range(num_epochs):\n",
    "    running_loss = 0.0\n",
    "    for i, data in enumerate(trainloader, 0):\n",
    "        inputs, labels = data\n",
    "\n",
    "        inputs = inputs.to(device)\n",
    "        labels = labels.to(device)\n",
    "        # Zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = net(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "\n",
    "        # Backward pass and optimize\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Print statistics\n",
    "        running_loss += loss.item()\n",
    "        if i % 100 == 99:  # Print every 100 mini-batches\n",
    "            print(f'Epoch [{epoch + 1}, {i + 1:5d}] loss: {running_loss / 100:.3f}')\n",
    "            running_loss = 0.0\n",
    "\n",
    "print('Finished Training')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the network on the 10000 test images: 96.88 %\n"
     ]
    }
   ],
   "source": [
    "correct = 0\n",
    "total = 0\n",
    "with torch.no_grad():\n",
    "    for data in testloader:\n",
    "        inputs, labels = data     #Should add this line\n",
    "        images = inputs.to(device)\n",
    "        labels = labels.to(device)\n",
    "        outputs = net(images)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "print(f'Accuracy of the network on the 10000 test images: {100 * correct / total} %')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
